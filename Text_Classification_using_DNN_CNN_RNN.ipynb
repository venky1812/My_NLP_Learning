{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification using DNN_CNN_RNN.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/venky1812/My_NLP_Learning/blob/main/Text_Classification_using_DNN_CNN_RNN.ipynb",
      "authorship_tag": "ABX9TyNNu/4kYYapoPcost+k1flj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venky1812/My_NLP_Learning/blob/main/Text_Classification_using_DNN_CNN_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srDqojrbj_-5"
      },
      "source": [
        "#from google.colab import drive\r\n",
        "\r\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q81XuFfOKGK8"
      },
      "source": [
        "# Text Classificatiion pipeline using DNN\r\n",
        "\r\n",
        "1.Tokenize the texts and convert them into word index vectors.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "2.Pad the text sequences so that all text vectors are of the same length.\r\n",
        "\r\n",
        "3.Map every word index to an embedding vector. We do that by multiplying word index vectors with the embedding matrix. The embedding matrix can either be populated using pre-trained embeddings or it can be trained for embeddings on this corpus.\r\n",
        "\r\n",
        "4.Use the output from Step 3 as the input to a neural network architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ww_yTuFxxqV"
      },
      "source": [
        "#import shutil\r\n",
        "#shutil.unpack_archive(\"drive/MyDrive/ml_dl_datasets/aclImdb_v1.tar.gz\", \"drive/MyDrive/ml_dl_datasets/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0c98zf46Ofo"
      },
      "source": [
        "#!unzip \"drive/MyDrive/ml_dl_datasets/imdb_sentiment_analysis/IMDB Dataset.csv.zip\" -d \"drive/MyDrive/ml_dl_datasets/imdb_sentiment_analysis/\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RYqdin2yOMp"
      },
      "source": [
        "import os\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "from tensorflow.keras.utils import to_categorical\r\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\r\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.initializers import Constant"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlpUNH0NAdza"
      },
      "source": [
        "# Setting seed value for reproducibility "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCJiiymJAcZg"
      },
      "source": [
        "tf.random.set_seed(1234)\r\n",
        "np.random.seed(1234)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDpV1e-32RCg"
      },
      "source": [
        "Defining dataset directory paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTefIom0Hrg"
      },
      "source": [
        "base_data_dir  = \"drive/MyDrive/ml_dl_datasets/\"\r\n",
        "\r\n",
        "DATA_DIR = os.path.join(base_data_dir, 'imdb_sentiment_analysis/IMDB Dataset.csv') #source: http://ai.stanford.edu/~amaas/data/sentiment/\r\n",
        "#TEST_DATA_DIR = os.path.join(base_data_dir, 'imdb_sentiment_analysis/test')\r\n",
        "\r\n",
        "## Hyperparameters \r\n",
        "\r\n",
        "MAX_SEQUENCE_LENGTH = 1000\r\n",
        "MAX_NUM_WORDS = 20000 \r\n",
        "EMBEDDING_DIM = 100 \r\n",
        "VALIDATION_SPLIT = 0.2\r\n",
        "\r\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0uNzaTG5sS6"
      },
      "source": [
        "Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJyDXpE80H7S"
      },
      "source": [
        "def get_data(data_dir):\r\n",
        "  print(data_dir)\r\n",
        "  texts = [] # list of text samples\r\n",
        "  labels = [] # list of text\r\n",
        "  label_index ={'pos':1,'neg':0}\r\n",
        "  for name in sorted(os.listdir(data_dir)):\r\n",
        "    if name != 'unsup':\r\n",
        "      path = os.path.join(data_dir,name)\r\n",
        "      if os.path.isdir(path):\r\n",
        "        label_id = label_index[name]\r\n",
        "        print(path)\r\n",
        "        for fname in sorted(os.listdir(path)):\r\n",
        "          fpath = os.path.join(path, fname)\r\n",
        "          text = open(fpath).read()\r\n",
        "          texts.append(text)\r\n",
        "          labels.append(label_id)\r\n",
        "\r\n",
        "  return texts,labels\r\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huBpxB127lVf",
        "outputId": "86683fe8-2ad1-4622-9c21-28f17dd9e767"
      },
      "source": [
        "dataset = pd.read_csv(DATA_DIR)\r\n",
        "\r\n",
        "cat_map = {'positive':1,'negative':0}\r\n",
        "\r\n",
        "dataset['sentiment'] = dataset['sentiment'].map(cat_map) \r\n",
        "\r\n",
        "train_texts, train_labels = dataset[:40000]['review'].tolist() , dataset[:40000]['sentiment'].tolist()\r\n",
        "\r\n",
        "test_texts, test_labels = dataset[40000:]['review'].tolist() , dataset[40000:]['sentiment'].tolist()\r\n",
        "\r\n",
        "print(len(train_texts))\r\n",
        "print(len(train_labels))\r\n",
        "print(len(test_texts))\r\n",
        "print(len(test_labels))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40000\n",
            "40000\n",
            "10000\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCcFf9R59kxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb198469-4615-4c3f-aeba-a78df2ebf2ae"
      },
      "source": [
        "#Vectorize these text samples into a 2D integer tensor using Keras Tokenizer\r\n",
        "#Tokenizer is fit on training data only, and that is used to tokenize both train and test data.\r\n",
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS,oov_token = '<OOV>')\r\n",
        "tokenizer.fit_on_texts(train_texts)\r\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts) #Converting text to a vector of word indexes\r\n",
        "test_sequences = tokenizer.texts_to_sequences(test_texts)\r\n",
        "word_index = tokenizer.word_index # useful for preparing embedding matrix\r\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 112174 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeWUNZwJDrUP"
      },
      "source": [
        "### Applying Padding to make text sequences to same length\r\n",
        "trainvalid_data  = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "\r\n",
        "### converting labels to categorical\r\n",
        "trainvalid_labels = to_categorical(np.asarray(train_labels))\r\n",
        "test_labels = to_categorical(np.asarray(test_labels))\r\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZYMwvKs_lmG"
      },
      "source": [
        "# split the training data into a training set and a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoMdYsX0-szp",
        "outputId": "7e2aa36c-4eca-4ccd-d0af-085148c0535c"
      },
      "source": [
        "indices = np.arange(trainvalid_data.shape[0])\r\n",
        "np.random.shuffle(indices)\r\n",
        "trainvalid_data = trainvalid_data[indices]\r\n",
        "trainvalid_labels = trainvalid_labels[indices]\r\n",
        "num_validation_samples = int(VALIDATION_SPLIT * trainvalid_data.shape[0])\r\n",
        "x_train = trainvalid_data[:-num_validation_samples]\r\n",
        "y_train = trainvalid_labels[:-num_validation_samples]\r\n",
        "x_val = trainvalid_data[-num_validation_samples:]\r\n",
        "y_val = trainvalid_labels[-num_validation_samples:]\r\n",
        "#This is the data we will use for CNN and RNN training\r\n",
        "print('Splitting the train data into train and valid is done')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Splitting the train data into train and valid is done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q84jhh0bCIud",
        "outputId": "06928f73-6d93-48d6-ade0-75e1a17c3953"
      },
      "source": [
        "print(\"preparing the embedding matrix\")\r\n",
        "\r\n",
        "\r\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing the embedding matrix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDCaSHdWJ-VT"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mysmqJCfGLIr"
      },
      "source": [
        "def build_cnn_model(num_words,embedding_mode):\r\n",
        "\r\n",
        "  cnnmodel = Sequential()\r\n",
        "\r\n",
        "  if embedding_mode == 1 :\r\n",
        "    print(\"Need to build\")\r\n",
        "    #cnnmodel = cnnmodel.add(Embedding(num_words,EMBEDDING_DIM,\r\n",
        "    #                        embeddings_initializer=Constant(embedding_matrix),\r\n",
        "    #                        input_length=MAX_SEQUENCE_LENGTH,trainable=False))\r\n",
        "  else:\r\n",
        "\r\n",
        "    cnnmodel.add(Embedding(num_words,128))\r\n",
        "\r\n",
        "  cnnmodel.add(MaxPooling1D(5))\r\n",
        "  cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "  cnnmodel.add(MaxPooling1D(5))\r\n",
        "  cnnmodel.add(Conv1D(128, 5, activation='relu'))\r\n",
        "  cnnmodel.add(GlobalMaxPooling1D())\r\n",
        "  cnnmodel.add(Dense(128, activation='relu'))\r\n",
        "  cnnmodel.add(Dense(2, activation='softmax'))\r\n",
        "\r\n",
        "  cnnmodel.compile(loss='categorical_crossentropy',\r\n",
        "              optimizer='rmsprop',\r\n",
        "              metrics=['acc'])\r\n",
        "  return cnnmodel                         "
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nK0IPHtL0Gx",
        "outputId": "54bad077-6010-42d4-9b03-741fa975859a"
      },
      "source": [
        "\r\n",
        "embedding_model = build_cnn_model(num_words,0)\r\n",
        "#Train the model. Tune to validation set. \r\n",
        "embedding_model.fit(x_train, y_train,batch_size=128,epochs=8, validation_data=(x_val, y_val))\r\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.6336 - acc: 0.5926 - val_loss: 0.3111 - val_acc: 0.8696\n",
            "Epoch 2/8\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.2684 - acc: 0.8920 - val_loss: 0.2848 - val_acc: 0.8823\n",
            "Epoch 3/8\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.1679 - acc: 0.9402 - val_loss: 0.2867 - val_acc: 0.8878\n",
            "Epoch 4/8\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.1020 - acc: 0.9639 - val_loss: 0.3049 - val_acc: 0.8930\n",
            "Epoch 5/8\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0593 - acc: 0.9808 - val_loss: 0.3990 - val_acc: 0.8836\n",
            "Epoch 6/8\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0260 - acc: 0.9918 - val_loss: 0.4056 - val_acc: 0.8867\n",
            "Epoch 7/8\n",
            "250/250 [==============================] - 16s 64ms/step - loss: 0.0154 - acc: 0.9946 - val_loss: 0.6020 - val_acc: 0.8871\n",
            "Epoch 8/8\n",
            "250/250 [==============================] - 16s 63ms/step - loss: 0.0084 - acc: 0.9973 - val_loss: 0.6898 - val_acc: 0.8839\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff73b4064e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_jXFVzIMMus",
        "outputId": "0ccffed5-f24b-4043-a3c2-e91cfd04a387"
      },
      "source": [
        "score, acc = embedding_model.evaluate(test_data, test_labels)\r\n",
        "print('Test accuracy with CNN:', acc)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 0.6348 - acc: 0.8897\n",
            "Test accuracy with CNN: 0.8896999955177307\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsq_obviOXPF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}